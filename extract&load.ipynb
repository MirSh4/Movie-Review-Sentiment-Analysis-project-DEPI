{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: psycopg2 in c:\\users\\lomah\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (2.9.9)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: C:\\Users\\lomah\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install psycopg2\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import getpass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Input credentials in a hidden manner\n",
    "hostname = input(\"Enter the hostname: \")\n",
    "database = input(\"Enter the database name: \")\n",
    "username = input(\"Enter the username: \")\n",
    "pwd = getpass.getpass(\"Enter the password: \")  # Password will be hidden\n",
    "port_id = input(\"Enter the port (default 5432): \") or \"5432\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Connect to the database\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        host=hostname,\n",
    "        dbname=database,\n",
    "        user=username,\n",
    "        password=pwd,\n",
    "        port=port_id\n",
    "    )\n",
    "    print(\"Connected to the database successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}, try connecting again\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "file_path = 'C:/Users/lomah/Desktop/FCAI/Activities&Tracks/ITIDA-NTI/data engineer/GP/data/title.ratings.tsv'\n",
    "\n",
    "\n",
    "# Step 1: Read the TSV file into a pandas DataFrame\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "print(\"\\nData Description:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nNumber of Null Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Step 2: Data Cleaning\n",
    "# Remove duplicates based on 'tconst'\n",
    "df = df.drop_duplicates(subset='tconst')\n",
    "\n",
    "\n",
    "output_path = 'C:/Users/lomah/Desktop/FCAI/Activities&Tracks/ITIDA-NTI/data engineer/GP/data after clean/cleaned_title_ratings.csv'  # Replace with your desired path\n",
    "df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#principles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "file_path = 'C:/Users/lomah/Desktop/FCAI/Activities&Tracks/ITIDA-NTI/data engineer/GP/data/title.principals.tsv'\n",
    "\n",
    "\n",
    "# Step 1: Read the TSV file into a pandas DataFrame\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "print(\"\\nData Description:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nNumber of Null Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Step 2: Data Cleaning\n",
    "# Drop rows with any missing values in 'tconst', 'averageRating', or 'numVotes'\n",
    "df = df.dropna(subset=['tconst', 'ordering', 'nconst', 'category'])\n",
    "\n",
    "# Remove duplicates based on 'tconst', 'nconst', and 'category'\n",
    "df = df.drop_duplicates(subset=['tconst', 'nconst', 'category'])\n",
    "\n",
    "# Step 3: Data Transformation\n",
    "plt.figure(figsize=(12, 6))\n",
    "df['category'].value_counts().plot(kind='bar', edgecolor='black')\n",
    "plt.title('Distribution of Job Categories')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "df['ordering'].hist(bins=20, edgecolor='black')\n",
    "plt.title('Distribution of Ordering')\n",
    "plt.xlabel('Ordering')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Path to the cleaned CSV file\n",
    "output_file_path = 'C:/Users/lomah/Desktop/FCAI/Activities&Tracks/ITIDA-NTI/data engineer/GP/2nd dataClean/principle2.csv'\n",
    "\n",
    "\n",
    "# Step 1: Read the original CSV in chunks\n",
    "chunk_size = 1000\n",
    "cleaned_chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(input_file_path, chunksize=chunk_size):\n",
    "    \n",
    "    # Function to clean and process the 'characters' column\n",
    "    def process_characters(characters):\n",
    "        if pd.notna(characters) and characters != '\\\\N':\n",
    "            try:\n",
    "                # Safely evaluate the string to a Python list\n",
    "                evaluated = ast.literal_eval(characters)\n",
    "                # If it's a list and contains at least one value, take the first one\n",
    "                if isinstance(evaluated, list) and len(evaluated) > 0:\n",
    "                    return evaluated[0]\n",
    "                else:\n",
    "                    return None\n",
    "            except (ValueError, SyntaxError):\n",
    "                # In case it's not a list or the evaluation fails, return None\n",
    "                return None\n",
    "        else:\n",
    "            return None  # Return None for NaN or '\\N' values\n",
    "\n",
    "    # Apply the character processing function to clean the data\n",
    "    if 'characters' in chunk.columns:\n",
    "        chunk['characters'] = chunk['characters'].apply(process_characters)\n",
    "\n",
    "    # Append cleaned chunk to list\n",
    "    cleaned_chunks.append(chunk)\n",
    "\n",
    "# Concatenate all chunks and save the cleaned data to a new CSV file\n",
    "cleaned_df = pd.concat(cleaned_chunks, ignore_index=True)\n",
    "cleaned_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "try:\n",
    "    # Connect to PostgreSQL\n",
    "    conn = psycopg2.connect(\n",
    "        host=hostname,\n",
    "        dbname=database,\n",
    "        user=username,\n",
    "        password=pwd,\n",
    "        port=port_id\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Open the CSV file and use the COPY command to load it into PostgreSQL\n",
    "    with open(output_file_path, 'r') as f:\n",
    "        next(f)  # Skip the header row\n",
    "\n",
    "        # Ensure that the table name matches the table in PostgreSQL\n",
    "        cur.copy_from(f, 'title_principals', sep=',', null='\\\\N',\n",
    "                      columns=('tconst', 'ordering', 'nconst', 'category', 'job'))\n",
    "\n",
    "    # Commit the transaction\n",
    "    conn.commit()\n",
    "    print(\"Data uploaded successfully.\")\n",
    "\n",
    "except Exception as error:\n",
    "    print(f\"Error while uploading the data to PostgreSQL: {error}\")\n",
    "\n",
    "finally:\n",
    "    if conn:\n",
    "        cur.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "file_path = 'C:/Users/lomah/Desktop/FCAI/Activities&Tracks/ITIDA-NTI/data engineer/GP/data/title.episode.tsv'\n",
    "\n",
    "\n",
    "# Step 1: Read the TSV file into a pandas DataFrame\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "print(\"\\nData Description:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nNumber of Null Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Step 2: Data Cleaning\n",
    "# Drop rows with any missing values in 'tconst', 'averageRating', or 'numVotes'\n",
    "df = df.dropna(subset=['tconst', 'parentTconst', 'seasonNumber', 'episodeNumber'])\n",
    "\n",
    "# Remove duplicates based on 'tconst'\n",
    "df = df.drop_duplicates(subset=['tconst'])\n",
    "\n",
    "\n",
    "\n",
    "output_path = 'C:/Users/lomah/Desktop/FCAI/Activities&Tracks/ITIDA-NTI/data engineer/GP/data after clean/cleaned_title_episode.csv'  # Replace with your desired path\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Description:\n",
      "           tconst directors   writers\n",
      "count    10477194  10477194  10477194\n",
      "unique   10477194    922554   1372100\n",
      "top     tt0000001        \\N        \\N\n",
      "freq            1   3989841   4640424\n",
      "\n",
      "Number of Null Values:\n",
      "tconst       0\n",
      "directors    0\n",
      "writers      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "file_path = 'C:/Users/lomah/Desktop/FCAI/Activities&Tracks/ITIDA-NTI/data engineer/GP/data/title.crew.tsv'\n",
    "\n",
    "\n",
    "# Step 1: Read the TSV file into a pandas DataFrame\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "print(\"\\nData Description:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nNumber of Null Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Step 2: Data Cleaning\n",
    "# Drop rows with any missing values \n",
    "df = df.dropna(subset=['tconst', 'directors', 'writers'])\n",
    "\n",
    "# Remove duplicates based on 'tconst'\n",
    "df = df.drop_duplicates(subset=['tconst'])\n",
    "\n",
    "\n",
    "\n",
    "output_path = 'C:/Users/lomah/Desktop/FCAI/Activities&Tracks/ITIDA-NTI/data engineer/GP/first data cleaned/data after clean/cleaned_title_crew2.csv'  # Replace with your desired path\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#akas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read the TSV.GZ file into a pandas DataFrame\n",
    "file_path = 'C:/Users/lomah/Desktop/FCAI/Activities&Tracks/ITIDA-NTI/data engineer/GP/data/title.akas.tsv'\n",
    "df = pd.read_csv(file_path, sep='\\t', compression='gzip')\n",
    "\n",
    "print(\"\\nData Description:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nNumber of Null Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Step 2: Data Cleaning\n",
    "# Drop rows with any missing values in essential columns\n",
    "df_cleaned = df.dropna(subset=['titleId', 'ordering', 'title', 'region', 'language', 'types'])\n",
    "\n",
    "\n",
    "# Remove duplicates based on 'titleId' and 'ordering'\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['titleId', 'ordering'])\n",
    "\n",
    "# Save the cleaned data to a CSV file (optional)\n",
    "output_path = 'C:/Users/lomah/Desktop/FCAI/Activities&Tracks/ITIDA-NTI/data engineer/GP/first data cleaned/data after clean/cleaned_title_akas.csv'\n",
    "df_cleaned.to_csv(output_path, index=False)\n",
    "print(f\"Cleaned data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/lomah/Desktop/FCAI/Activities&Tracks/ITIDA-NTI/data engineer/GP/first data cleaned/data after clean/cleaned_title_akas.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Clean data: Remove specific characters from all string columns\n",
    "df = df.applymap(lambda x: x.replace('\"', '').replace(\"'\", '').replace('{', '').replace('}', '').replace('\\n', ' ') if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "# Connect to the database\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        host=hostname,\n",
    "        dbname=database,\n",
    "        user=username,\n",
    "        password=pwd,\n",
    "        port=port_id\n",
    "    )\n",
    "    print(\"Connected to the database successfully\")\n",
    "    \n",
    "    # Create a cursor object\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Iterate over DataFrame rows and insert each record into the table\n",
    "    for _, row in df.iterrows():\n",
    "        # Convert types and attributes to array format if they are not null\n",
    "        types = \"{\" + row['types'] + \"}\" if pd.notna(row['types']) else None\n",
    "        attributes = \"{\" + row['attributes'] + \"}\" if pd.notna(row['attributes']) else None\n",
    "        \n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO title_akas (titleId, ordering, title, region, language, types, attributes, isOriginalTitle)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "            \"\"\",\n",
    "            (\n",
    "                row['titleId'],\n",
    "                row['ordering'],\n",
    "                row['title'],\n",
    "                row['region'],\n",
    "                row['language'],\n",
    "                types,\n",
    "                attributes,\n",
    "                row['isOriginalTitle']\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Commit the changes and close the cursor and connection\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    print(\"Data inserted successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}, try connecting again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the TSV.GZ file into a pandas DataFrame\n",
    "file_path = 'C:/Users/lomah/Desktop/FCAI/Activities&Tracks/ITIDA-NTI/data engineer/GP/data/name.basics.tsv.gz'\n",
    "df = pd.read_csv(file_path, sep='\\t', compression='gzip')\n",
    "\n",
    "print(\"\\nData Description:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nNumber of Null Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Step 2: Data Cleaning\n",
    "# Drop rows with missing essential fields\n",
    "df_cleaned = df.dropna(subset=['nconst', 'primaryName', 'primaryProfession', 'knownForTitles'])\n",
    "\n",
    "# Replace any unwanted characters in text columns (like newlines)\n",
    "df_cleaned['primaryName'] = df_cleaned['primaryName'].replace({'\\n': ' '}, regex=True)\n",
    "df_cleaned['primaryProfession'] = df_cleaned['primaryProfession'].replace({'\\n': ' '}, regex=True)\n",
    "df_cleaned['knownForTitles'] = df_cleaned['knownForTitles'].replace({'\\n': ' '}, regex=True)\n",
    "\n",
    "# Optional: Replace missing values in numeric columns with NULL\n",
    "df_cleaned['birthYear'] = df_cleaned['birthYear'].replace({'\\\\N': None})\n",
    "df_cleaned['deathYear'] = df_cleaned['deathYear'].replace({'\\\\N': None})\n",
    "\n",
    "# Remove duplicates based on 'nconst'\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=['nconst'])\n",
    "\n",
    "# Save cleaned data to a CSV (optional)\n",
    "output_path = 'C:/Users/lomah/Desktop/FCAI/Activities&Tracks/ITIDA-NTI/data engineer/GP/first data cleaned/cleaned_name_basics.csv'\n",
    "df_cleaned.to_csv(output_path, index=False)\n",
    "print(f\"Cleaned data saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "# File path for the CSV data\n",
    "file_path = 'C:/Users/lomah/Desktop/FCAI/Activities&Tracks/ITIDA-NTI/data engineer/GP/first data cleaned/data after clean/cleaned_title_basics.csv'\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Step 3: Convert 'isAdult' to boolean type (True for 1, False for 0)\n",
    "df['isAdult'] = df['isAdult'].astype(bool)\n",
    "\n",
    "# Step 4: Remove duplicate rows based on 'tconst' column\n",
    "df.drop_duplicates(subset=['tconst'], inplace=True)\n",
    "\n",
    "\n",
    "try:\n",
    "    # Establish the connection\n",
    "    conn = psycopg2.connect(\n",
    "        host=hostname,\n",
    "        dbname=database,\n",
    "        user=username,\n",
    "        password=pwd,\n",
    "        port=port_id\n",
    "    )\n",
    "    print(\"Connected to the database successfully\")\n",
    "\n",
    "    # Create a cursor object\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Create the 'name_basics' table if it doesn't exist\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS name_basics (\n",
    "            nconst VARCHAR(20) PRIMARY KEY,\n",
    "            primaryName VARCHAR(255),\n",
    "            birthYear INT NULL,\n",
    "            deathYear INT NULL,\n",
    "            primaryProfession TEXT,\n",
    "            knownForTitles TEXT\n",
    "        )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "\n",
    "    # Step 4: Insert data into PostgreSQL table\n",
    "    for _, row in df_cleaned.iterrows():\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO name_basics (nconst, primaryName, birthYear, deathYear, primaryProfession, knownForTitles)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s)\n",
    "            \"\"\",\n",
    "            (\n",
    "                row['nconst'],\n",
    "                row['primaryName'],\n",
    "                row['birthYear'],\n",
    "                row['deathYear'],\n",
    "                row['primaryProfession'],\n",
    "                row['knownForTitles']\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Commit the transaction and close the connection\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    print(\"Data inserted successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}, try connecting again\")\n",
    "\n",
    "    # Read CSV in chunks and insert data\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        chunk['isAdult'] = chunk['isAdult'].astype(bool)  # Convert isAdult to boolean if necessary\n",
    "        for index, row in chunk.iterrows():\n",
    "            cur.execute(insert_query, (\n",
    "                row['tconst'], row['titleType'], row['primaryTitle'], row['originalTitle'],\n",
    "                row['isAdult'], row['startYear'], row['endYear'], row['runtimeMinutes'], row['genres']\n",
    "            ))\n",
    "\n",
    "        # Commit the changes after each chunk\n",
    "        conn.commit()\n",
    "    \n",
    "    print(\"Data inserted successfully.\")\n",
    "\n",
    "except Exception as error:\n",
    "    print(f\"Error: {error}\")\n",
    "\n",
    "finally:\n",
    "    if conn:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
